#!/usr/bin/env python3
"""Export trained model to GGUF format for Ollama deployment.

Converts the merged HuggingFace model to GGUF, quantizes it,
creates an Ollama Modelfile, and registers it with `ollama create`.

Usage:
    python export_gguf.py --model-dir /output/math-qlora/merged-model
    python export_gguf.py --model-dir /output/math-qlora/merged-model --quant-type q5_k_m
    python export_gguf.py --model-dir /output/math-qlora/merged-model --model-name morningstar-math-v2

Author: Ali Nasser
"""

import argparse
import shutil
import subprocess
import sys
from pathlib import Path

# ---------------------------------------------------------------------------
# Constants
# ---------------------------------------------------------------------------
SUPPORTED_QUANTS = ["q4_k_m", "q5_k_m", "q8_0", "f16"]
DEFAULT_QUANT = "q4_k_m"
DEFAULT_MODEL_NAME = "morningstar-math"

SYSTEM_PROMPT = (
    "Du bist MORNINGSTAR-MATH -- der maechtigste Mathematik-Assistent.\n"
    "Entwickelt von Ali Nasser.\n\n"
    "REGELN:\n"
    "1. Denke IMMER Schritt fuer Schritt\n"
    "2. Zeige ALLE Rechenwege\n"
    "3. Setze die finale Antwort in \\\\boxed{}\n"
    "4. Pruefe deine Ergebnisse durch Gegenrechnung\n"
    "5. Bei Unsicherheit: Versuche alternative Loesungswege"
)

MODELFILE_TEMPLATE = '''FROM ./{gguf_filename}

TEMPLATE "{{{{ if .System }}}}<|im_start|>system
{{{{ .System }}}}<|im_end|>
{{{{ end }}}}{{{{ if .Prompt }}}}<|im_start|>user
{{{{ .Prompt }}}}<|im_end|>
{{{{ end }}}}<|im_start|>assistant
{{{{ .Response }}}}<|im_end|>
"

SYSTEM """
{system_prompt}
"""

PARAMETER temperature 0.3
PARAMETER top_p 0.9
PARAMETER num_ctx 4096
PARAMETER stop "<|im_end|>"
PARAMETER stop "<|im_start|>"
'''


# ---------------------------------------------------------------------------
# Export Methods
# ---------------------------------------------------------------------------
def export_with_unsloth(model_dir: str, output_dir: str, quant_type: str) -> Path:
    """Export to GGUF using Unsloth's built-in conversion (preferred)."""
    print(f"Exporting with Unsloth to {quant_type.upper()}...")

    import torch
    from unsloth import FastLanguageModel

    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name=model_dir,
        max_seq_length=2048,
        dtype=torch.bfloat16,
        load_in_4bit=False,
    )

    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    model.save_pretrained_gguf(
        str(output_path),
        tokenizer,
        quantization_method=quant_type,
    )

    gguf_files = list(output_path.glob("*.gguf"))
    if not gguf_files:
        print("ERROR: No GGUF file generated by Unsloth.")
        sys.exit(1)

    gguf_path = gguf_files[0]
    size_gb = gguf_path.stat().st_size / 1024**3
    print(f"GGUF file created: {gguf_path} ({size_gb:.2f} GB)")
    return gguf_path


def export_with_llama_cpp(model_dir: str, output_dir: str, quant_type: str) -> Path:
    """Fallback: Export using llama.cpp's convert scripts."""
    print(f"Exporting with llama.cpp to {quant_type.upper()}...")

    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    # Step 1: Convert to F16 GGUF
    f16_path = output_path / "model-f16.gguf"
    print("Step 1/2: Converting to F16 GGUF...")

    convert_script = (
        shutil.which("convert-hf-to-gguf.py")
        or shutil.which("convert_hf_to_gguf.py")
    )

    if convert_script:
        convert_cmd = [
            sys.executable, convert_script,
            "--outfile", str(f16_path),
            "--outtype", "f16",
            str(model_dir),
        ]
    else:
        convert_cmd = [
            sys.executable, "-m", "llama_cpp.convert",
            "--outfile", str(f16_path),
            "--outtype", "f16",
            str(model_dir),
        ]

    result = subprocess.run(convert_cmd, capture_output=True, text=True)
    if result.returncode != 0:
        print(f"ERROR: F16 conversion failed:\n{result.stderr}")
        print("Hint: Install llama-cpp-python or use Unsloth method.")
        sys.exit(1)

    if quant_type == "f16":
        return f16_path

    # Step 2: Quantize
    quant_map = {"q4_k_m": "Q4_K_M", "q5_k_m": "Q5_K_M", "q8_0": "Q8_0"}
    quant_flag = quant_map.get(quant_type, quant_type.upper())
    quantized_path = output_path / f"model-{quant_type}.gguf"

    print(f"Step 2/2: Quantizing to {quant_flag}...")
    quantize_bin = shutil.which("llama-quantize") or shutil.which("quantize")
    if not quantize_bin:
        print("ERROR: llama-quantize not found. Use Unsloth method instead.")
        sys.exit(1)

    result = subprocess.run(
        [quantize_bin, str(f16_path), str(quantized_path), quant_flag],
        capture_output=True, text=True,
    )
    if result.returncode != 0:
        print(f"ERROR: Quantization failed:\n{result.stderr}")
        sys.exit(1)

    f16_path.unlink(missing_ok=True)
    size_gb = quantized_path.stat().st_size / 1024**3
    print(f"GGUF file created: {quantized_path} ({size_gb:.2f} GB)")
    return quantized_path


# ---------------------------------------------------------------------------
# Ollama Integration
# ---------------------------------------------------------------------------
def create_modelfile(gguf_path: Path, output_dir: str) -> Path:
    """Generate an Ollama Modelfile."""
    modelfile_path = Path(output_dir) / "Modelfile"
    content = MODELFILE_TEMPLATE.format(
        gguf_filename=gguf_path.name,
        system_prompt=SYSTEM_PROMPT,
    )
    modelfile_path.write_text(content, encoding="utf-8")
    print(f"Modelfile written to: {modelfile_path}")
    return modelfile_path


def register_with_ollama(modelfile_path: Path, model_name: str) -> bool:
    """Register the model with Ollama."""
    ollama_bin = shutil.which("ollama")
    if not ollama_bin:
        print(f"WARNING: ollama not found. Register manually:")
        print(f"  cd {modelfile_path.parent} && ollama create {model_name} -f Modelfile")
        return False

    print(f"\nRegistering model as '{model_name}' in Ollama...")
    result = subprocess.run(
        [ollama_bin, "create", model_name, "-f", str(modelfile_path)],
        cwd=str(modelfile_path.parent),
        capture_output=True,
        text=True,
    )

    if result.returncode != 0:
        print(f"WARNING: ollama create failed:\n{result.stderr}")
        print(f"Register manually: cd {modelfile_path.parent} && "
              f"ollama create {model_name} -f Modelfile")
        return False

    print(f"Model '{model_name}' registered successfully.")
    return True


# ---------------------------------------------------------------------------
# Main
# ---------------------------------------------------------------------------
def main() -> None:
    parser = argparse.ArgumentParser(
        description="Export trained model to GGUF for Ollama"
    )
    parser.add_argument("--model-dir", type=str, required=True,
                        help="Path to merged HuggingFace model directory")
    parser.add_argument("--output-dir", type=str, default=None,
                        help="Output directory (default: <model-dir>/../gguf)")
    parser.add_argument("--quant-type", type=str, default=DEFAULT_QUANT,
                        choices=SUPPORTED_QUANTS,
                        help=f"Quantization type (default: {DEFAULT_QUANT})")
    parser.add_argument("--model-name", type=str, default=DEFAULT_MODEL_NAME,
                        help=f"Ollama model name (default: {DEFAULT_MODEL_NAME})")
    parser.add_argument("--all-quants", action="store_true",
                        help="Export all quantization variants")
    parser.add_argument("--skip-ollama", action="store_true",
                        help="Skip Ollama registration")
    parser.add_argument("--method", type=str, default="unsloth",
                        choices=["unsloth", "llama-cpp"],
                        help="Export method (default: unsloth)")
    args = parser.parse_args()

    model_dir = Path(args.model_dir)
    if not model_dir.exists():
        print(f"ERROR: Model directory not found: {model_dir}")
        sys.exit(1)

    output_dir = args.output_dir or str(model_dir.parent / "gguf")
    quant_types = ["q4_k_m", "q5_k_m", "q8_0"] if args.all_quants else [args.quant_type]
    export_fn = export_with_unsloth if args.method == "unsloth" else export_with_llama_cpp

    print("=" * 60)
    print("  GGUF Export Pipeline")
    print(f"  Source:     {model_dir}")
    print(f"  Output:     {output_dir}")
    print(f"  Quant(s):   {', '.join(q.upper() for q in quant_types)}")
    print(f"  Method:     {args.method}")
    print(f"  Model name: {args.model_name}")
    print("=" * 60 + "\n")

    primary_gguf = None
    for quant in quant_types:
        print(f"\n--- Exporting {quant.upper()} ---")
        gguf_path = export_fn(str(model_dir), output_dir, quant)
        if primary_gguf is None:
            primary_gguf = gguf_path

    if primary_gguf:
        modelfile_path = create_modelfile(primary_gguf, output_dir)
        if not args.skip_ollama:
            register_with_ollama(modelfile_path, args.model_name)

    print("\n" + "=" * 60)
    print("Export complete!")
    print(f"  GGUF files: {output_dir}")
    if not args.skip_ollama:
        print(f"\nTest with: ollama run {args.model_name}")
    print("=" * 60)


if __name__ == "__main__":
    main()
